---
title: Annotation Curricula to Implicitly Train Non-Expert Annotators
authors:
- Ji-Ung Lee
- Jan-Christoph Klie
- Iryna Gurevych
date: '2022-06-01'
publishDate: '2023-11-18T15:30:38.998626Z'
publication_types:
- article-journal
publication: '*Computational Linguistics*'
doi: 10.1162/coli_a_00436
abstract: Annotation studies often require annotators to familiarize themselves with
  the task, its annotation scheme, and the data domain. This can be overwhelming in
  the beginning, mentally taxing, and induce errors into the resulting annotations;
  especially in citizen science or crowdsourcing scenarios where domain expertise
  is not required. To alleviate these issues, this work proposes annotation curricula,
  a novel approach to implicitly train annotators. The goal is to gradually introduce
  annotators into the task by ordering instances to be annotated according to a learning
  curriculum. To do so, this work formalizes annotation curricula for sentence- and
  paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing
  heuristics and interactively trained models on three existing English datasets.
  Finally, we provide a proof of concept for annotation curricula in a carefully designed
  user study with 40 voluntary participants who are asked to identify the most fitting
  misconception for English tweets about the Covid-19 pandemic. The results indicate
  that using a simple heuristic to order instances can already significantly reduce
  the total annotation time while preserving a high annotation quality. Annotation
  curricula thus can be a promising research direction to improve data collection.
  To facilitate future research—for instance, to adapt annotation curricula to specific
  tasks and expert annotation scenarios—all code and data from the user study consisting
  of 2,400 annotations is made available.1
links:
- name: URL
  url: https://doi.org/10.1162/coli_a_00436
---
